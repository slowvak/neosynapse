<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Understanding and addressing algorithmic bias in medical AI. By Dr. Bradley J. Erickson at mdsynapse.org">
    <title>When Algorithms Inherit Bias: Fairness Challenges in Medical AI | NeoSynapse.md</title>

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://mdsynapse.org/algorithm-bias.html">
    <meta property="og:title" content="When Algorithms Inherit Bias: Fairness Challenges in Medical AI">
    <meta property="og:description" content="The algorithm is 92% accurate for white patients and 78% accurate for Black patients. Do you use it? A physician's guide to recognizing and addressing bias.">
    <meta property="og:image" content="https://mdsynapse.org/og-image-bias.png">
    <meta property="article:author" content="Dr. Bradley J. Erickson">
    <meta property="article:published_time" content="2025-12-27">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://mdsynapse.org/algorithm-bias.html">
    <meta property="twitter:title" content="When Algorithms Inherit Bias: Fairness Challenges in Medical AI">
    <meta property="twitter:description" content="The algorithm is 92% accurate for white patients and 78% accurate for Black patients. Do you use it?">
    <meta property="twitter:image" content="https://mdsynapse.org/og-image-bias.png">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://mdsynapse.org/algorithm-bias.html">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="article-styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Space+Grotesk:wght@500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Navigation -->
    <nav class="nav">
        <div class="container">
            <div class="nav-content">
                <a href="index.html" class="logo">
                    <div class="logo-mark">
                        <svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <circle cx="16" cy="8" r="2.5" fill="#3FD1C1"/>
                            <circle cx="8" cy="16" r="2.5" fill="#3FD1C1" opacity="0.7"/>
                            <circle cx="24" cy="16" r="2.5" fill="#3FD1C1" opacity="0.7"/>
                            <circle cx="16" cy="24" r="2.5" fill="#3FD1C1"/>
                            <circle cx="16" cy="16" r="3" fill="#3FD1C1" class="pulse"/>
                            <line x1="16" y1="10.5" x2="16" y2="13" stroke="#3FD1C1" stroke-width="1.5"/>
                            <line x1="16" y1="19" x2="16" y2="21.5" stroke="#3FD1C1" stroke-width="1.5"/>
                            <line x1="10.5" y1="16" x2="13" y2="16" stroke="#3FD1C1" stroke-width="1.5"/>
                            <line x1="19" y1="16" x2="21.5" y2="16" stroke="#3FD1C1" stroke-width="1.5"/>
                        </svg>
                    </div>
                    <span class="logo-text">NeoSynapse<span class="logo-ext">.md</span></span>
                </a>
                <div class="nav-links">
                    <a href="index.html">Home</a>
                    <a href="about.html">About</a>
                    <a href="index.html#articles">Thoughts on Medical AI</a>
                    <a href="papers-of-note.html">Papers of Note</a>
                    <a href="index.html#mission">Mission</a>
                </div>
            </div>
        </div>
    </nav>

    <!-- Article Header -->
    <article class="article">
        <header class="article-header">
            <div class="container">
                <div class="article-header-content">
                    <div class="article-tag tag-ethics">Ethics</div>
                    <h1 class="article-header-title">When Algorithms Inherit Bias: Fairness Challenges in Medical AI</h1>
                    <p class="article-header-subtitle">The algorithm is 92% accurate for white patients and 78% accurate for Black patients. Do you use it? A physician's guide to recognizing and addressing bias in clinical AI.</p>
                    <div class="article-meta-header">
                        <span class="article-date">December 27, 2025</span>
                        <span class="article-divider">•</span>
                        <span class="article-reading-time">9 min read</span>
                        <span class="article-divider">•</span>
                        <span class="article-author">By <a href="about.html" style="color: var(--teal-primary); text-decoration: none;">Dr. Bradley J. Erickson</a></span>
                    </div>
                </div>
            </div>
        </header>

        <!-- Article Content -->
        <div class="article-body">
            <div class="container">
                <div class="article-content">

                    <p class="lead">
                        You're reviewing a new AI tool for your hospital. The validation study shows 90% accuracy overall. Impressive. But buried in the supplementary materials, you notice: 92% for white patients, 78% for Black patients.
                    </p>

                    <p>
                        Do you adopt it?
                    </p>

                    <p>
                        This isn't a hypothetical. It's happening in health systems across the country—algorithms that work brilliantly for some patients and fail dangerously for others. Not because developers intended harm, but because bias in healthcare data is invisible until you look for it.
                    </p>

                    <blockquote class="article-quote">
                        "The algorithm isn't biased. The system that created the data is biased. The algorithm just learned what we taught it."
                    </blockquote>

                    <h2>Why Bias in Medical AI Is Different</h2>

                    <p>
                        Bias in healthcare AI isn't like bias in social media ads or loan approvals. It's not just unfair—it's <em>dangerous</em>. Because when an algorithm systematically under-serves a population, people get sicker. They get misdiagnosed. They don't get referred for life-saving interventions. They lose trust in the system and disengage, leading to less data for future AI, leading to potentially greater disparities.
                    </p>

                    <p>
                        And unlike other domains, healthcare bias is particularly insidious because:
                    </p>

                    <h3>Healthcare Data Reflects Historical Inequities</h3>

                    <p>
                        Medical records aren't neutral. They encode centuries of disparities:
                    </p>

                    <ul>
                        <li><strong>Access barriers:</strong> Underserved populations have fewer encounters, fewer tests, less documentation</li>
                        <li><strong>Diagnostic bias:</strong> Symptoms described differently across demographics (e.g., chest pain vs. "anxious," shortness of breath vs. "panic")</li>
                        <li><strong>Treatment disparities:</strong> Different populations receive different interventions for the same conditions</li>
                    </ul>

                    <p>
                        When you train AI on this data, it doesn't see bias. It sees <em>patterns</em>. And it optimizes for those patterns—including the biased ones. Machine learning algorithms, whether traditional models or deep neural networks, learn relationships in training data without understanding the social context that created those relationships.<sup><a href="#ref10" style="color: var(--teal-primary);">10</a></sup>
                    </p>

                    <h3>"Objective" Algorithms Trained on Subjective Systems</h3>

                    <p>
                        We assume algorithms are neutral because they're mathematical. But every algorithm is trained on data generated by humans making subjective decisions:
                    </p>

                    <ul>
                        <li>Which patients get referred for specialist care</li>
                        <li>Which symptoms get documented vs. dismissed</li>
                        <li>Which tests get ordered (and which don't)</li>
                        <li>How pain is assessed and believed</li>
                    </ul>

                    <p>
                        Example: An AI trained to predict "who needs a cardiology consult" will learn from historical referral patterns. If cardiologists historically under-referred women and minorities for chest pain, the AI will replicate that bias—because that's the "ground truth" it was trained on.
                    </p>

                    <p>
                        The algorithm isn't wrong. The <em>labels</em> are wrong.
                    </p>

                    <h2>Where Bias Enters the Pipeline</h2>

                    <p>
                        Bias doesn't come from a single source. It accumulates across the entire AI development lifecycle—from data handling through model development to performance evaluation. Understanding where bias enters is the first step toward mitigation.<sup><a href="#ref7" style="color: var(--teal-primary);">7-9</a></sup>
                    </p>

                    <h3>1. Dataset Composition: Who Is Represented?</h3>

                    <p>
                        Most medical AI is trained on data from academic medical centers, which systematically underrepresent:
                    </p>

                    <ul>
                        <li>Rural populations</li>
                        <li>Non-English speakers</li>
                        <li>Specific racial and ethnic minorities</li>
                    </ul>

                    <p>
                        <strong>Result:</strong> Models perform worse on populations they haven't seen during training. This data composition bias, arising from non-representative sampling during data collection, is one of the fundamental sources of algorithmic inequity.<sup><a href="#ref7" style="color: var(--teal-primary);">7</a></sup>
                    </p>

                    <p>
                        Example: A diabetic retinopathy screening AI trained predominantly on images from white patients missed significant pathology in Black and Hispanic patients due to differences in fundus pigmentation and image acquisition parameters.
                    </p>

                    <h3>2. Labeling Bias: What Gets Called "Disease"?</h3>

                    <p>
                        Ground truth labels are often proxies for actual disease:
                    </p>

                    <ul>
                        <li><strong>Diagnosis codes:</strong> Reflect what was documented and billed, not necessarily what exists</li>
                        <li><strong>Lab thresholds:</strong> Reference ranges derived from non-representative populations</li>
                        <li><strong>Procedure use:</strong> "Who got an MRI" ≠ "who needed an MRI"</li>
                    </ul>

                    <p>
                        <strong>The Obermeyer Problem:</strong> A widely-used algorithm predicted healthcare needs based on <em>cost</em>—assuming sicker patients cost more. But Black patients receive less care (and thus cost less) for the same severity of illness. So the algorithm systematically classified Black patients as "less sick" than equally ill white patients.<sup><a href="#ref1" style="color: var(--teal-primary);">1</a></sup>
                    </p>

                    <p>
                        The model worked perfectly. The labels were biased. Careful label investigation and validation against clinical ground truth—rather than administrative proxies—is essential to avoid perpetuating these disparities.<sup><a href="#ref7" style="color: var(--teal-primary);">7</a></sup>
                    </p>

                    <h3>3. Proxy Variables: Race as a Shortcut</h3>

                    <p>
                        Some algorithms use race explicitly. Others use proxies:
                    </p>

                    <ul>
                        <li>ZIP code (redlining proxy)</li>
                        <li>Insurance status (socioeconomic proxy)</li>
                        <li>Primary language (immigration status proxy)</li>
                        <li>Hospital of admission (geographic segregation proxy)</li>
                    </ul>

                    <p>
                        Even when race isn't in the model, <em>everything correlated with race</em> can recreate bias.
                    </p>

                    <div class="article-callout">
                        <h4>The eGFR Controversy</h4>
                        <p>
                            For decades, kidney function equations included a "race correction factor"—giving Black patients a higher estimated GFR for the same creatinine level. The assumption: Black patients have more muscle mass.
                        </p>
                        <p>
                            <strong>Result:</strong> Black patients were systematically delayed in referral for transplant, dialysis, and nephrology care.
                        </p>
                        <p>
                            In 2021, medical societies finally recommended that race be removed from eGFR calculations—but millions of patients were likely harmed in the interim.
                        </p>
                        <p>
                            <strong>Lesson:</strong> "Biologically justified" adjustments can still cause harm when they're based on flawed assumptions about race.
                        </p>
                    </div>

                    <h2>Real Clinical Examples: Bias at the Bedside</h2>

                    <h3>Race-Based Algorithms in Nephrology</h3>

                    <p>
                        Beyond eGFR, race appears in algorithms for:
                    </p>

                    <ul>
                        <li><strong>Kidney stone risk:</strong> Lower predicted risk for Black patients (despite similar disease prevalence)</li>
                        <li><strong>Preeclampsia screening:</strong> Different thresholds by race for proteinuria</li>
                        <li><strong>Urinary tract infection diagnosis:</strong> Race-adjusted prediction rules</li>
                    </ul>

                    <p>
                        The common thread: Using race as a biological variable when it's actually a <em>social</em> variable capturing disparities in care, environment, and discrimination.
                    </p>

                    <h3>Pulse Oximetry + AI: Compounding Error</h3>

                    <p>
                        Pulse oximeters are less accurate in patients with darker skin—overestimating oxygen saturation by 2-3% on average.
                    </p>

                    <p>
                        Now imagine training an AI on pulse oximetry data to predict:
                    </p>

                    <ul>
                        <li>Who needs supplemental oxygen</li>
                        <li>Who should be admitted to ICU</li>
                        <li>Who meets criteria for ECMO</li>
                    </ul>

                    <p>
                        The AI learns from <em>systematically inaccurate readings</em> in Black patients. Result: It underestimates disease severity and under-triages care.
                    </p>

                    <p>
                        This isn't theoretical—it happened during COVID-19, when pulse oximetry-based AI triage systems contributed to delayed care for minority patients with hypoxemia.
                    </p>

                    <h3>Under-Triage of Minority Patients</h3>

                    <p>
                        Triage algorithms predict who needs higher-acuity care. But if trained on historical patterns where:
                    </p>

                    <ul>
                        <li>Minority patients are less likely to be admitted to ICU (due to bias or structural barriers)</li>
                        <li>Pain is under-recognized and under-treated in Black patients</li>
                        <li>Women's cardiac symptoms are more likely to be dismissed</li>
                    </ul>

                    <p>
                        Then the AI will replicate those patterns—flagging fewer minority and female patients as "high-risk" even when clinically equivalent to white male patients.
                    </p>

                    <h2>Why Clinicians Are Often the Last to Know</h2>

                    <h3>Bias Is Invisible at Point of Care</h3>

                    <p>
                        When you use an AI tool, you see:
                    </p>

                    <ul>
                        <li>This patient's prediction</li>
                        <li>This patient's recommendation</li>
                    </ul>

                    <p>
                        You <em>don't</em> see:
                    </p>

                    <ul>
                        <li>How predictions vary across demographics</li>
                        <li>False negative rates by race</li>
                        <li>Whether the model underperforms for your population vs. the validation cohort</li>
                    </ul>

                    <p>
                        Bias is a <em>population-level</em> phenomenon. Individual physicians can't detect it from individual cases.
                    </p>

                    <h3>Lack of Transparency in Commercial Tools</h3>

                    <p>
                        Most hospital-purchased AI tools don't disclose:
                    </p>

                    <ul>
                        <li>Training data demographics</li>
                        <li>Subgroup performance metrics</li>
                        <li>Disparate impact assessments</li>
                        <li>Bias mitigation strategies employed</li>
                    </ul>

                    <p>
                        You're expected to trust that "the algorithm works"—but you have no way to verify fairness.
                    </p>

                    <h2>What Fairness Means Clinically</h2>

                    <p>
                        "Fairness" sounds simple. It's not. There are multiple, mathematically incompatible definitions—and the choice of performance metrics used to evaluate fairness fundamentally shapes what "success" looks like.<sup><a href="#ref9" style="color: var(--teal-primary);">9</a></sup>
                    </p>

                    <h3>Group Fairness (Demographic Parity)</h3>

                    <p>
                        <strong>Definition:</strong> The algorithm predicts the same rate of positive outcomes across groups.
                    </p>

                    <p>
                        <strong>Example:</strong> If 10% of white patients are flagged as "high-risk," then 10% of Black patients should also be flagged.
                    </p>

                    <p>
                        <strong>Problem:</strong> If disease prevalence differs across groups (due to social determinants, not biology), forcing equal rates means the algorithm will <em>under-predict</em> in the higher-prevalence group.
                    </p>

                    <h3>Individual Fairness (Equal Treatment)</h3>

                    <p>
                        <strong>Definition:</strong> Similar patients get similar predictions, regardless of group membership.
                    </p>

                    <p>
                        <strong>Example:</strong> Two patients with identical vitals, labs, and history get the same risk score—whether they're Black, white, male, female, etc.
                    </p>

                    <p>
                        <strong>Problem:</strong> "Similar" is subjective. If the definition of "similar" ignores structural inequities (e.g., access to care, neighborhood safety, environmental exposures), you're comparing apples to oranges.
                    </p>

                    <h3>Equalized Odds (Equal Accuracy)</h3>

                    <p>
                        <strong>Definition:</strong> The model has the same sensitivity and specificity across groups.
                    </p>

                    <p>
                        <strong>Example:</strong> Among patients who truly have disease, the model catches 85% regardless of race. Among patients who don't, it correctly rules out 90% regardless of race.
                    </p>

                    <p>
                        <strong>Problem:</strong> Achieving this mathematically often requires different decision thresholds for different groups—which raises its own ethical questions.
                    </p>

                    <div class="article-callout">
                        <h4>The Fairness Impossibility Theorem</h4>
                        <p>
                            You <strong>cannot</strong> simultaneously achieve perfect fairness. Optimizing for one group often worsens others.
                        </p>
                        <p>
                            This means <strong>there is no "unbiased" algorithm</strong> in a mathematical sense. There are only <em>different tradeoffs</em>.
                        </p>
                        <p>
                            The question isn't "Is this fair?" It's "Which type of fairness matters most for this clinical use case—and who bears the cost of our choice?"
                        </p>
                    </div>

                    <h3>Tradeoffs Between Accuracy and Equity</h3>

                    <p>
                        Sometimes, optimizing for fairness reduces overall accuracy.
                    </p>

                    <p>
                        Example: A sepsis predictor is 90% accurate overall but only 82% accurate for Hispanic patients (due to sparse training data). You can:
                    </p>

                    <ul>
                        <li><strong>Option A:</strong> Use the model as-is (90% overall, disparate performance)</li>
                        <li><strong>Option B:</strong> Retrain to equalize performance across groups (maybe 87% overall, 87% for all groups)</li>
                        <li><strong>Option C:</strong> Don't deploy—collect better data first</li>
                    </ul>

                    <p>
                        There's no obviously "right" answer. It depends on your values:
                    </p>

                    <ul>
                        <li>Maximize benefit for the majority? (Option A)</li>
                        <li>Equalize outcomes across groups, even if average drops? (Option B)</li>
                        <li>Wait until you can do both? (Option C—but patients suffer in the interim)</li>
                    </ul>

                    <h2>Why "Removing Race" Isn't Enough</h2>

                    <p>
                        The instinctive response to race-based bias: <em>Just don't use race as a variable.</em>
                    </p>

                    <p>
                        Problem: Race is correlated with <em>everything</em> in healthcare data.
                    </p>

                    <ul>
                        <li>ZIP code</li>
                        <li>Hospital of admission</li>
                        <li>Insurance type</li>
                        <li>Comorbidity burden (driven by access, not biology)</li>
                        <li>Lab values (affected by different baseline health states)</li>
                    </ul>

                    <p>
                        Remove race from the model, and the algorithm will reconstruct it from these proxies. This is called <strong>redundant encoding</strong>—when group membership can be inferred from other variables. Model selection, architecture choices, and training techniques all influence how these patterns get encoded.<sup><a href="#ref8" style="color: var(--teal-primary);">8</a></sup>
                    </p>

                    <h3>Case Study: Predicting No-Shows</h3>

                    <p>
                        A hospital builds an AI to predict which patients will miss appointments (to overbook strategically).
                    </p>

                    <p>
                        They <em>don't</em> use race. But they use:
                    </p>

                    <ul>
                        <li>Missed appointments in the past (correlated with transportation barriers, work schedule inflexibility—both racialized)</li>
                        <li>Insurance type (Medicaid heavily racialized due to historical policy)</li>
                        <li>Neighborhood (segregation proxy)</li>
                    </ul>

                    <p>
                        Result: The model flags Black and Hispanic patients as "high no-show risk" at higher rates. Clinic staff assume they're unreliable. They get less flexible scheduling, more restrictive policies, worse rapport.
                    </p>

                    <p>
                        The algorithm never saw race. But it learned racism anyway.
                    </p>

                    <h2>Practical Guidance for Physicians</h2>

                    <h3>Questions to Ask Vendors</h3>

                    <p>
                        Before your hospital adopts an AI tool, demand answers:
                    </p>

                    <ol>
                        <li><strong>What was the demographic composition of the training data?</strong><br>If they can't tell you, that's a red flag.<sup><a href="#ref7" style="color: var(--teal-primary);">7</a></sup></li>
                        <li><strong>What is the model's performance across racial/ethnic subgroups?</strong><br>Overall accuracy means nothing if it's 95% for white patients and 75% for Black patients. Demand stratified metrics—sensitivity, specificity, PPV, NPV—by demographic group.<sup><a href="#ref9" style="color: var(--teal-primary);">9</a></sup></li>
                        <li><strong>Were disparate impact assessments performed?</strong><br>Did you test whether the model systematically over- or under-predicts for specific groups?</li>
                        <li><strong>Does the model use race as a variable?</strong><br>If yes, why? What's the biological justification (vs. social determinant)?<sup><a href="#ref2" style="color: var(--teal-primary);">2</a></sup></li>
                        <li><strong>How do you handle missing data?</strong><br>Minority patients often have sparser EHR data. How does the model account for this?<sup><a href="#ref7" style="color: var(--teal-primary);">7</a></sup></li>
                        <li><strong>Was external validation performed on diverse populations?</strong><br>Internal testing on the same institution's data isn't enough. Models must be tested on external datasets representing different demographics and practice settings.<sup><a href="#ref9" style="color: var(--teal-primary);">9</a></sup></li>
                        <li><strong>What's your plan for ongoing bias monitoring?</strong><br>Performance should be tracked by subgroup post-deployment, with clear thresholds for intervention when disparities emerge.</li>
                    </ol>

                    <h3>Warning Signs of Biased Systems</h3>

                    <p>
                        Red flags that an algorithm may be biased:
                    </p>

                    <ul>
                        <li>Validation study doesn't report subgroup performance</li>
                        <li>Training data is from a single health system or region</li>
                        <li>Model uses "cost" or "utilization" as a proxy for illness severity</li>
                        <li>Vendor can't explain what features drive predictions</li>
                        <li>No plan for monitoring fairness post-deployment</li>
                    </ul>

                    <p>
                        If vendors dismiss fairness concerns as "not relevant to our model," walk away.
                    </p>

                    <h3>The Clinician's Ethical Role</h3>

                    <p>
                        You are the <em>final common pathway</em> for AI in medicine. Even if the algorithm is biased, you can mitigate harm by:
                    </p>

                    <ul>
                        <li><strong>Questioning predictions that don't match clinical judgment</strong> — especially for patients from underrepresented groups</li>
                        <li><strong>Documenting when you override the AI</strong> — this creates the data needed to detect bias</li>
                        <li><strong>Advocating for better tools</strong> — demand fairness assessments before procurement</li>
                        <li><strong>Reporting disparate outcomes</strong> — if minority patients seem to have worse outcomes with AI-assisted care, raise it</li>
                    </ul>

                    <p>
                        The algorithm can't be racist. But the <em>system</em> can be. And you're part of the system.
                    </p>

                    <hr class="article-divider-line">

                    <h2>Moving Forward: What Responsible AI Looks Like</h2>

                    <p class="article-conclusion">
                        Bias in medical AI isn't a technical problem with a technical solution. It's a justice problem that requires ongoing vigilance, transparency, and accountability.
                    </p>

                    <p class="article-conclusion">
                        We can't eliminate bias entirely—it's baked into the data we learn from. But we can <em>acknowledge it, measure it, and mitigate it</em>.
                    </p>

                    <p class="article-conclusion">
                        That means:
                    </p>

                    <ul style="font-size: 1.125rem; color: var(--navy-dark);">
                        <li>Demanding transparency from vendors on the data properties used to train algorithms</li>
                        <li>Testing algorithms on diverse populations before deployment</li>
                        <li>Monitoring performance by subgroup after deployment</li>
                        <li>Being willing to retire tools that exacerbate disparities</li>
                        <li>Centering equity in AI governance, not treating it as an afterthought</li>
                    </ul>

                    <p class="article-conclusion">
                        The algorithm that's 92% accurate for white patients and 78% for Black patients?
                    </p>

                    <p class="article-conclusion">
                        Don't adopt it. Not until it works equally well for everyone.
                    </p>

                    <p class="article-conclusion">
                        Because the cost of that 14-point gap isn't borne by the algorithm. It's borne by the patients we're supposed to serve.
                    </p>

                    <hr class="article-divider-line">

                    <!-- Key Takeaways -->
                    <div class="article-callout">
                        <h4>Key Takeaways</h4>
                        <ul style="margin: 0; padding-left: 1.5rem;">
                            <li><strong>Bias Reflects Data, Not Intent:</strong> AI learns from historical healthcare data that encodes systemic inequities—access barriers, diagnostic bias, treatment disparities.</li>
                            <li><strong>Labels Are Often Proxies:</strong> "Ground truth" based on who got care (not who needed it) perpetuates disparities. Cost ≠ illness severity.</li>
                            <li><strong>Removing Race Isn't Enough:</strong> Algorithms reconstruct race from proxies (ZIP code, insurance, hospital). Redundant encoding is pervasive.</li>
                            <li><strong>Multiple Definitions of Fairness:</strong> Group fairness, individual fairness, and equalized odds are mathematically incompatible. Tradeoffs are inevitable.</li>
                            <li><strong>Demand Transparency:</strong> Ask vendors: What's the training data demographics? Subgroup performance? Disparate impact assessment? Ongoing monitoring plan?</li>
                            <li><strong>Physicians Are Gatekeepers:</strong> You can mitigate bias by questioning predictions, documenting overrides, and advocating for equitable tools.</li>
                        </ul>
                    </div>

                    <!-- References -->
                    <h2>References & Further Reading</h2>

                    <ol class="article-references" style="font-size: 0.95rem; line-height: 1.7; color: var(--gray-dark);">
                        <li id="ref1">
                            Obermeyer Z, Powers B, Vogeli C, Mullainathan S. <strong>Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.</strong> <em>Science.</em> 2019;366(6464):447-453.
                            <a href="https://doi.org/10.1126/science.aax2342" target="_blank" style="color: var(--teal-primary);">doi:10.1126/science.aax2342</a>
                        </li>
                        <li id="ref2">
                            Vyas DA, Eisenstein LG, Jones DS. <strong>Hidden in Plain Sight — Reconsidering the Use of Race Correction in Clinical Algorithms.</strong> <em>N Engl J Med.</em> 2020;383(9):874-882.
                            <a href="https://doi.org/10.1056/NEJMms2004740" target="_blank" style="color: var(--teal-primary);">doi:10.1056/NEJMms2004740</a>
                        </li>
                        <li id="ref3">
                            Sjoding MW, Dickson RP, Iwashyna TJ, Gay SE, Valley TS. <strong>Racial Bias in Pulse Oximetry Measurement.</strong> <em>N Engl J Med.</em> 2020;383(25):2477-2478.
                            <a href="https://doi.org/10.1056/NEJMc2029240" target="_blank" style="color: var(--teal-primary);">doi:10.1056/NEJMc2029240</a>
                        </li>
                        <li id="ref4">
                            FDA. <strong>Artificial Intelligence and Machine Learning in Software as a Medical Device.</strong> Updated 2023.
                            <a href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device" target="_blank" style="color: var(--teal-primary);">FDA.gov</a>
                        </li>
                        <li id="ref5">
                            World Health Organization. <strong>Ethics and Governance of Artificial Intelligence for Health.</strong> WHO Guidance. 2021.
                            <a href="https://www.who.int/publications/i/item/9789240029200" target="_blank" style="color: var(--teal-primary);">WHO.int</a>
                        </li>
                        <li id="ref6">
                            Gianfrancesco MA, Tamang S, Yazdany J, Schmajuk G. <strong>Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data.</strong> <em>JAMA Intern Med.</em> 2018;178(11):1544-1547.
                        </li>
                        <li id="ref7">
                            Rouzrokh P, Wyles CC, Philbrick KA, Ramazanian T, Weston AD, Cai JC, Taunton MJ, Kremers WK, Lewallen DG, Erickson BJ. <strong>A Deep Learning Tool for Automated Radiographic Measurement of Acetabular Component Inclination and Version After Total Hip Arthroplasty. Part 1: Mitigating Bias in Machine Learning—Data Handling.</strong> <em>J Arthroplasty.</em> 2022;37(6S):S406-S413.
                            <a href="https://doi.org/10.1016/j.arth.2022.02.092" target="_blank" style="color: var(--teal-primary);">doi:10.1016/j.arth.2022.02.092</a>
                        </li>
                        <li id="ref8">
                            Zhang Y, Wyles CC, Makhni MC, Maradit Kremers H, Sellon JL, Erickson BJ. <strong>Part 2: Mitigating Bias in Machine Learning—Model Development.</strong> <em>J Arthroplasty.</em> 2022;37(6S):S414-S420.
                            <a href="https://doi.org/10.1016/j.arth.2022.02.085" target="_blank" style="color: var(--teal-primary);">doi:10.1016/j.arth.2022.02.085</a>
                        </li>
                        <li id="ref9">
                            Faghani S, Khosravi B, Moassefi M, Rouzrokh P, Erickson BJ. <strong>Part 3: Mitigating Bias in Machine Learning—Performance Metrics, Healthcare Applications, and Fairness in Machine Learning.</strong> <em>J Arthroplasty.</em> 2022;37(6S):S421-S428.
                            <a href="https://doi.org/10.1016/j.arth.2022.02.087" target="_blank" style="color: var(--teal-primary);">doi:10.1016/j.arth.2022.02.087</a>
                        </li>
                        <li id="ref10">
                            Erickson BJ, Kitamura F. <strong>Artificial Intelligence in Radiology: a Primer for Radiologists.</strong> <em>Radiol Clin North Am.</em> 2021;59(6):991-1003.
                            <a href="https://doi.org/10.1016/j.rcl.2021.07.004" target="_blank" style="color: var(--teal-primary);">doi:10.1016/j.rcl.2021.07.004</a>
                        </li>
                    </ol>

                </div>

                <!-- Article Footer -->
                <footer class="article-footer">
                    <div class="article-tags">
                        <span class="tag">Artificial Intelligence</span>
                        <span class="tag">Algorithmic Bias</span>
                        <span class="tag">Health Equity</span>
                        <span class="tag">Clinical Ethics</span>
                        <span class="tag">Social Justice</span>
                    </div>

                    <div class="article-share">
                        <p>Share this article:</p>
                        <div class="share-buttons">
                            <a href="#" class="share-btn" aria-label="Share on Twitter">Twitter</a>
                            <a href="#" class="share-btn" aria-label="Share on LinkedIn">LinkedIn</a>
                            <a href="#" class="share-btn" aria-label="Copy link">Copy Link</a>
                        </div>
                    </div>

                    <div class="article-cta">
                        <h3>Want more insights like this?</h3>
                        <p>Deep thinking on medicine, AI, and the future clinician—delivered to your inbox.</p>
                        <a href="index.html#connect" class="btn btn-primary">Stay Connected</a>
                    </div>
                </footer>
            </div>
        </div>
    </article>

    <!-- Related Articles -->
    <section class="related-articles">
        <div class="container">
            <h2 class="section-title">Continue Reading</h2>
            <div class="articles-grid">
                <article class="article-card">
                    <div class="article-tag tag-ai">AI</div>
                    <h3 class="article-title">From Lab to Ward: Why Most Medical AI Fails at Deployment</h3>
                    <p class="article-excerpt">The validation study was flawless. The deployment was a disaster. Here's why impressive AI tools never make it to the bedside—and what successful implementation actually looks like.</p>
                    <div class="article-meta">
                        <span class="read-time">8 min read</span>
                        <a href="from-lab-to-ward.html" class="article-link">Read more →</a>
                    </div>
                </article>

                <article class="article-card">
                    <div class="article-tag tag-ai">AI</div>
                    <h3 class="article-title">Model Drift in Medicine: When AI Quietly Gets Worse</h3>
                    <p class="article-excerpt">In 2019, your sepsis model had 89% sensitivity. Today it's 71%. Nobody told you. Nobody checked. The silent patient safety crisis you've never heard of.</p>
                    <div class="article-meta">
                        <span class="read-time">7 min read</span>
                        <a href="model-drift.html" class="article-link">Read more →</a>
                    </div>
                </article>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-bottom">
                    <div class="footer-links">
                        <a href="index.html">Home</a>
                        <a href="about.html">About</a>
                        <a href="index.html#articles">Thoughts on Medical AI</a>
                        <a href="papers-of-note.html">Papers of Note</a>
                        <a href="index.html#mission">Mission</a>
                    </div>
                </div>
                <div class="footer-copyright">
                    <p>&copy; 2025 NeoSynapse.md | Published at <a href="https://mdsynapse.org" style="color: var(--teal-primary); text-decoration: none;">mdsynapse.org</a></p>
                </div>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
