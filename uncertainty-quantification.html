<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Why AI needs to communicate doubt: the critical role of uncertainty quantification in medical decision-making. By Dr. Bradley J. Erickson at mdsynapse.org">
    <title>The Confidence Problem: Why Your AI Needs to Learn to Say "I Don't Know" | NeoSynapse.md</title>

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://mdsynapse.org/uncertainty-quantification.html">
    <meta property="og:title" content="The Confidence Problem: Why Your AI Needs to Learn to Say 'I Don't Know'">
    <meta property="og:description"
        content="The algorithm says 85% confident—but is it? Understanding why uncertainty quantification is essential for trustworthy medical AI.">
    <meta property="og:image" content="https://mdsynapse.org/og-image-uncertainty.png">
    <meta property="article:author" content="Dr. Bradley J. Erickson">
    <meta property="article:published_time" content="2026-01-01">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://mdsynapse.org/uncertainty-quantification.html">
    <meta property="twitter:title" content="The Confidence Problem: Why Your AI Needs to Learn to Say 'I Don't Know'">
    <meta property="twitter:description"
        content="The algorithm says 85% confident—but is it? Understanding why uncertainty quantification is essential for trustworthy medical AI.">
    <meta property="twitter:image" content="https://mdsynapse.org/og-image-uncertainty.png">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://mdsynapse.org/uncertainty-quantification.html">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="article-styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Space+Grotesk:wght@500;600;700&display=swap"
        rel="stylesheet">
</head>

<body>
    <!-- Navigation -->
    <nav class="nav">
        <div class="container">
            <div class="nav-content">
                <a href="index.html" class="logo">
                    <div class="logo-mark">
                        <svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <circle cx="16" cy="8" r="2.5" fill="#3FD1C1" />
                            <circle cx="8" cy="16" r="2.5" fill="#3FD1C1" opacity="0.7" />
                            <circle cx="24" cy="16" r="2.5" fill="#3FD1C1" opacity="0.7" />
                            <circle cx="16" cy="24" r="2.5" fill="#3FD1C1" />
                            <circle cx="16" cy="16" r="3" fill="#3FD1C1" class="pulse" />
                            <line x1="16" y1="10.5" x2="16" y2="13" stroke="#3FD1C1" stroke-width="1.5" />
                            <line x1="16" y1="19" x2="16" y2="21.5" stroke="#3FD1C1" stroke-width="1.5" />
                            <line x1="10.5" y1="16" x2="13" y2="16" stroke="#3FD1C1" stroke-width="1.5" />
                            <line x1="19" y1="16" x2="21.5" y2="16" stroke="#3FD1C1" stroke-width="1.5" />
                        </svg>
                    </div>
                    <span class="logo-text">NeoSynapse<span class="logo-ext">.md</span></span>
                </a>
                <div class="nav-links">
                    <a href="index.html">Home</a>
                    <a href="about.html">About</a>
                    <a href="index.html#articles">Thoughts on Medical AI</a>
                    <a href="papers-of-note.html">Papers of Note</a>
                    <a href="index.html#mission">Mission</a>
                </div>
            </div>
        </div>
    </nav>

    <!-- Article Header -->
    <article class="article">
        <header class="article-header">
            <div class="container">
                <div class="article-header-content">
                    <div class="article-tag tag-ai">AI</div>
                    <h1 class="article-header-title">The Confidence Problem: Why Your AI Needs to Learn to Say "I Don't
                        Know"</h1>
                    <p class="article-header-subtitle">The algorithm says 85% probability of pneumonia. But how
                        confident is it in that 85%? Without uncertainty quantification, every AI prediction is a false
                        promise waiting to undermine trust.</p>
                    <div class="article-meta-header">
                        <span class="article-date">January 1, 2026</span>
                        <span class="article-divider">•</span>
                        <span class="article-reading-time">11 min read</span>
                        <span class="article-divider">•</span>
                        <span class="article-author">By <a href="about.html"
                                style="color: var(--teal-primary); text-decoration: none;">Dr. Bradley J.
                                Erickson</a></span>
                    </div>
                </div>
            </div>
        </header>

        <!-- Article Content -->
        <div class="article-body">
            <div class="container">
                <div class="article-content">

                    <p class="lead">
                        Your worklist shows a notification: "Possible PE detected—prioritize for review." You click on
                        the study. The AI triage tool has flagged it, but provides no probability, no confidence score,
                        no indication of <em>which</em> images triggered the alert. Just a binary flag: positive or
                        negative. Finding present or not.
                    </p>

                    <p>
                        This isn't a research prototype. This is the reality of most FDA-cleared AI devices deployed in
                        radiology today. They're triage tools—designed to flag studies for priority review or route them
                        to specific worklists. And the vast majority provide nothing beyond a binary decision because
                        that is what the FDA clearance allows.
                    </p>

                    <p>
                        When the AI is wrong—and it will be wrong—you have no way to know <em>how confident</em> it was
                        about that wrong answer. A growth plate or nutrient canal gets misclassified as a fracture gets
                        the same binary "fracture detected" flag as a obvious displaced femur fracture. The triage
                        system treats them identically.
                    </p>

                    <blockquote class="article-quote">
                        "An AI that can't communicate uncertainty is like a doctor who says you are disease free or
                        not—whether they're certain, concerned, a bit puzzled or just guessing."
                    </blockquote>

                    <h2>The Binary Triage Problem</h2>

                    <p>
                        Most FDA-cleared AI devices function as triage tools. They analyze imaging studies in the
                        background and generate binary alerts:
                    </p>

                    <ul>
                        <li>"Intracranial hemorrhage detected"</li>
                        <li>"Large vessel occlusion"</li>
                        <li>"Pneumothorax detected"</li>
                        <li>"Rib fracture identified"</li>
                    </ul>

                    <p>
                        The study moves up on your worklist but critically, they provide <strong>no indication of
                            confidence</strong>. You don't get a probability. You don't get an uncertainty measure. You
                        often don't even get to see which image or which finding triggered the alert.
                    </p>

                    <p>
                        Behind the scenes, the model computed some internal score that crossed a threshold. But that
                        threshold is hidden from you. A case that barely exceeded the threshold (model internal score:
                        51%) gets the same "detected" flag as an obvious case (model internal score: 99%).
                    </p>

                    <p>
                        <strong>Result:</strong> Every positive alert is treated as equally urgent. You have no way to
                        know which alerts are reliable and which are edge cases. When the AI flags a growth plate as a
                        fracture, you get the same urgent notification as you do for a displaced femur fracture.
                    </p>


                    <h3>Why This Matters Clinically</h3>

                    <p>
                        Consider two scenarios with a pneumothorax triage tool:
                    </p>

                    <ul>
                        <li><strong>Case A:</strong> Large tension pneumothorax on an upright chest X-ray. Obvious to
                            any human observer. The AI model's internal score: 0.95 (highly confident).</li>
                        <li><strong>Case B:</strong> Skin fold artifact on a supine portable chest X-ray that vaguely
                            resembles a pneumothorax. The AI model's internal score: 0.52 (barely above threshold,
                            highly uncertain).</li>
                    </ul>

                    <p>
                        <strong>What you see in both cases:</strong> "Pneumothorax detected—exam moved to top of list."
                    </p>

                    <p>
                        Without uncertainty information, you treat both identically. You might immediately interrupt a
                        procedure to review Case B, only to find it's artifact. As a result, alert fatigue from false
                        positives means you might delay reviewing Case A.
                    </p>

                    <h2>What Uncertainty Quantification Actually Means</h2>

                    <p>
                        Uncertainty quantification (UQ) is the AI equivalent of a radiologist saying "I'm not sure about
                        this one."
                    </p>

                    <p>
                        Instead of a simple binary flag, a UQ-enabled triage system could provide:
                    </p>

                    <p>
                        Uncertainty represents the <em>trustworthiness</em> of the model's decision. High uncertainty
                        means the model doesn't have enough information to make a reliable call. Low uncertainty means
                        the prediction is likely trustworthy.<sup><a href="#ref1"
                                style="color: var(--teal-primary);">1</a></sup>
                    </p>

                    <p>
                        Most importantly, UQ would allow triage systems to communicate: "I flagged this, but I'm not
                        certain—please review carefully" versus "I flagged this and I'm highly confident—this is
                        urgent."
                    </p>

                    <h3>Two Types of Uncertainty</h3>

                    <p>
                        Understanding where uncertainty comes from helps us reduce it:
                    </p>

                    <ul>
                        <li><strong>Aleatoric uncertainty (data uncertainty):</strong> Irreducible noise in the data
                            itself. Some chest X-rays genuinely look identical but have different diagnoses based on
                            patient history. No amount of training data will eliminate this—it's inherent ambiguity.
                        </li>
                        <li><strong>Epistemic uncertainty (knowledge uncertainty):</strong> Uncertainty due to gaps in
                            the model's training. The model hasn't seen enough diverse examples. This <em>can</em> be
                            reduced by training on more varied data.<sup><a href="#ref1"
                                    style="color: var(--teal-primary);">1</a></sup></li>
                    </ul>

                    <p>
                        When a model flags high epistemic uncertainty, it's telling you: "I haven't seen enough cases
                        like this to be confident." That's valuable information—it tells you exactly when to intervene.
                    </p>

                    <h2>Beyond the Black Box: A 2026 Perspective on Uncertainty Quantification in AI</h2>

                    <h3>1. The Reliability Crisis: Why "Accuracy" Is No Longer Enough</h3>
                    <p>
                        For the better part of a decade, the artificial intelligence community has been locked in a
                        singular race: the pursuit of state-of-the-art (SOTA) accuracy. Whether pushing ImageNet scores
                        by a fraction of a percentage or dominating the MMLU leaderboard, the metric of success was
                        predictive performance.
                    </p>
                    <p>
                        As we settle into 2026, the focus is shifting and I believe that is critical for adoption in
                        healthcare. The deployment of Large Language Models (LLMs) and autonomous vision systems into a
                        high-stakes environment like healthcare has exposed a critical shortcoming. These models are
                        very good at some tasks and sound convincing, but they are also <strong>confident
                            sycophants</strong>. They frequently hallucinate with convincing language but fail to
                        recognize when data lies outside their training distribution, and struggle to communicate "I
                        don't know."
                    </p>
                    <p>
                        This post provides a deep dive into the current state of <strong>Uncertainty Quantification
                            (UQ)</strong>. Drawing from the latest research, it explores how the field is moving beyond
                        simple probability scores toward rigorous, distribution-free guarantees and semantic-aware
                        reliability.
                    </p>

                    <h3>2. The Computer Vision Revolution: Conformal Prediction Takes Center Stage</h3>
                    <p>
                        In the realm of computer vision, the era of heuristic calibration (like Temperature Scaling) is
                        fading. A strong candidate for addressing certainty is <strong>Conformal Prediction
                            (CP)</strong>. Unlike traditional Bayesian methods, which often rely on unprovable priors,
                        CP offers mathematically rigorous, finite-sample coverage guarantees.
                    </p>
                    <p>
                        However, standard CP relies on the <strong>exchangeability assumption</strong>—the idea that
                        tomorrow's test data will look statistically identical to yesterday's calibration data. In the
                        real world, this assumption rarely holds.
                    </p>

                    <h4>Breaking the Exchangeability Barrier: WQLCP</h4>
                    <p>
                        One recent approach to solving this "distribution shift" problem is
                        <strong>Weighted Quantile Loss-scaled Conformal Prediction (WQLCP)</strong>.<sup>1</sup>
                    </p>
                    <p>
                        Standard Conformal Prediction fails when the test distribution drifts (e.g., a self-driving car
                        moving from sunny California to snowy Toronto). WQLCP addresses this by dynamically scaling the
                        prediction sets based on the "weirdness" of the input.
                    </p>

                    <h2>Why Calibration Isn't Enough</h2>

                    <p>
                        Some might argue: "Just calibrate the model. If it says 80%, make sure 80% of those cases are
                        actually positive." Calibration is valuable—but it's not sufficient. Here's why:
                    </p>

                    <p>
                        Calibration ensures probabilities match observed frequencies <strong></strong>in the calibration
                        data set.</strong> If your model predicts 70% pneumonia for 100 cases, and 70 of those actually
                        have pneumonia, the model is calibrated.
                    </p>

                    <p>
                        <strong>But calibration is population-level.</strong> It doesn't tell you whether <em>this
                            specific prediction</em> is reliable. Even a perfectly calibrated model can be highly
                        uncertain or very confident about individual cases and that does not match the probability.<sup><a href="#ref1"
                                style="color: var(--teal-primary);">1</a></sup>
                    </p>

                    <p>
                        Consider two well-calibrated pneumonia models:
                    </p>

                    <ul>
                        <li><strong>Model A:</strong> Trained on many cases of <em>Klebsiella</em> pneumonia. Sees a
                            <em>Klebsiella</em> case, predicts 70% pneumonia with low uncertainty (0.05).</li>
                        <li><strong>Model B:</strong> Never trained on <em>Klebsiella</em> pneumonia. Sees the same
                            case, predicts 70% pneumonia with high uncertainty (0.6).</li>
                    </ul>

                    <p>
                        Both are calibrated. Both output 70%. But Model B is far less reliable for this particular
                        case because it had not seen Klebsiella before. Only uncertainty quantification reveals that.
                    </p>

                    <h3>Calibration Depends on Prevalence</h3>

                    <p>
                        Here's another problem: calibration isn't portable. A model calibrated on a population with 20%
                        disease prevalence needs recalibration if you deploy it in a setting with 60% prevalence.
                    </p>

                    <p>
                        This means you can't just trust vendor claims about calibration. You need to validate
                        calibration <em>on your own population</em>—and even then, you still need uncertainty
                        quantification to know which individual predictions are trustworthy.
                    </p>

                    <h2>How Uncertainty Quantification Works</h2>

                    <p>
                        There are several approaches to quantifying uncertainty in deep learning models. The most common
                        include:
                    </p>

                    <h3>1. Ensemble Methods</h3>

                    <p>
                        Train multiple models (with different initializations or architectures) on the same task. For
                        each new image, pass it through all models. If they all agree (e.g., all predict 80-85%
                        pneumonia), uncertainty is low. If they disagree widely (predictions range from 30% to 90%),
                        uncertainty is high.
                    </p>

                    <p>
                        <strong>Pros:</strong> Intuitive, effective<br>
                        <strong>Cons:</strong> Computationally expensive—you need to train and run multiple models
                    </p>

                    <h3>2. Bayesian and Drop Out Methods (Monte Carlo Dropout)</h3>

                    <p>
                        Run the same model multiple times on the same image, but randomly "drop out" different neurons
                        each time. This simulates having an ensemble without training multiple models. The variance in
                        predictions reflects uncertainty.
                    </p>

                    <p>
                        <strong>Pros:</strong> Single model, relatively efficient<br>
                        <strong>Cons:</strong> Still requires multiple forward passes at inference time<sup><a
                                href="#ref1" style="color: var(--teal-primary);">1</a></sup>
                    </p>

                    <h3>3. Evidential Deep Learning</h3>

                    <p>
                        Train the model to collect "evidence" for each class from the image features. The more evidence
                        collected, the lower the uncertainty. This method can output uncertainty in a single forward
                        pass.
                    </p>

                    <p>
                        <strong>Pros:</strong> Fast at inference, theoretically principled<br>
                        <strong>Cons:</strong> More complex to implement, less tested in medical imaging<sup><a
                                href="#ref1" style="color: var(--teal-primary);">1</a></sup>
                    </p>

                    <h2>Practical Applications of Uncertainty Quantification</h2>

                    <h3>Flagging Cases for Expert Review</h3>

                    <p>
                        The most straightforward use: if uncertainty exceeds a threshold, route the case to a
                        radiologist for careful review. High-certainty cases can be auto-validated or given lower
                        priority.
                    </p>

                    <p>
                        This creates a hybrid workflow where AI handles routine cases confidently, but defers to human
                        expertise for edge cases—exactly the collaboration we want.
                    </p>

                    <h3>Improving Model Performance</h3>

                    <p>
                        Uncertainty quantification can directly improve diagnostic accuracy. For segmentation tasks,
                        knowing which pixels the model is uncertain about allows for refinement.
                    </p>

                    <p>
                        One study showed that incorporating uncertainty into brain tumor segmentation improved Dice
                        coefficients by 3.15% for enhancing tumor and 0.58% for necrotic tumor—clinically meaningful
                        improvements for treatment planning.<sup><a href="#ref3"
                                style="color: var(--teal-primary);">3</a></sup>
                    </p>

                    <h3>Efficient Active Learning</h3>

                    <p>
                        As new data becomes available, you want to retrain your model. But labeling data is expensive.
                        Which new cases should radiologists annotate?
                    </p>

                    <p>
                        <strong>Answer:</strong> The ones the model is most uncertain about. These are the cases that
                        will teach the model the most. UQ enables efficient, targeted data collection instead of
                        randomly labeling everything.<sup><a href="#ref1"
                                style="color: var(--teal-primary);">1</a></sup>
                    </p>

                    <h3>Detecting Bias and Distribution Shift</h3>

                    <p>
                        High uncertainty can reveal when a model is being applied outside its training
                        distribution—which often indicates bias.
                    </p>

                    <p>
                        Example: A pediatric pneumonia model deployed on adult patients will show high epistemic
                        uncertainty on adult cases. That's a red flag that the model is unreliable for this population.
                        Without UQ, you'd only discover the problem through clinical errors.
                    </p>

                    <p>
                        As discussed in our <a href="algorithm-bias.html" style="color: var(--teal-primary);">AI bias
                            article</a>, models trained on non-representative populations propagate that bias
                        downstream. UQ can help detect when a model is uncertain specifically <em>because</em> it's
                        seeing a demographic it wasn't trained on.
                    </p>

                    <h2>What to Demand from Vendors</h2>

                    <p>
                        If you're evaluating AI triage tools for clinical use, here are essential questions about
                        uncertainty quantification:
                    </p>

                    <h3>1. Does the triage tool provide any confidence or uncertainty information with alerts?</h3>

                    <p>
                        Current FDA-cleared devices don't. They provide binary output: "positive" or "negative."
                    </p>

                    <p>
                        <strong>Minimum acceptable:</strong> Confidence levels (high/medium/low) displayed with each
                        alert<br>
                        <strong>Better:</strong> Quantitative uncertainty scores that you can use to set routing
                        rules<br>
                        <strong>Best:</strong> UQ methods that also show which image features drove the alert
                    </p>

                    <p>
                        <strong>Red flag:</strong> Vendor says "the model has 95% sensitivity, and all alerts are
                        equally urgent." That's population-level performance. It doesn't tell you which individual
                        alerts are reliable.
                    </p>

                    <h3>2. How is uncertainty quantified?</h3>

                    <p>
                        If they provide uncertainty at all, ask which method they use (ensemble, Bayesian, evidential,
                        etc.). Often the vendor will equate the output of the penulatimate layer as a probability (it isnt), or 
                        they will refer to the calibrated probability as the confidence (it isn't). Be sure youunderstand this.
                    </p>

                    <h3>3. What's the positive predictive value stratified by confidence level?</h3>

                    <p>
                        Don't accept overall PPV. Ask: among "high confidence" alerts, what's the PPV? Among "low
                        confidence" alerts? If they can't provide this, they haven't properly validated their
                        uncertainty quantification.
                    </p>

                    <p>
                        <strong>Example acceptable answer:</strong> "High confidence alerts have 65% PPV, low confidence
                        have 12% PPV in our validation study."
                    </p>

                    <h3>4. Can I configure alert behavior based on confidence?</h3>

                    <p>
                        Ideally, you should be able to set rules like: "High confidence alerts → urgent notification.
                        Low confidence → add to worklist but don't interrupt workflow." Vendors should provide tools to
                        customize this for your practice.
                    </p>

                    <h3>6. How does the tool handle out-of-distribution cases?</h3>

                    <p>
                        Ask: if the model sees a pediatric study but was trained on adults, does it flag high
                        uncertainty? If it encounters artifact or unusual positioning, does uncertainty increase?
                    </p>

                    <p>
                        If the vendor says "the model handles all cases equally well," that's impossible and suggests
                        they haven't thought about uncertainty at all.
                    </p>

                    <h2>The Path Forward: Building Trust Through Uncertainty</h2>

                    <p>
                        The paradox of AI in medicine is that models need to be both accurate <em>and</em> humble. An AI
                        that confidently misdiagnoses is worse than useless—it's dangerous and trust-destroying.
                    </p>

                    <p>
                        Uncertainty quantification solves this by giving AI systems a way to communicate doubt. When the
                        model says "I'm uncertain," it's not admitting failure—it's demonstrating trustworthiness. It's
                        saying: "This case is outside my expertise. A human should look at this."
                    </p>

                    <p>
                        That kind of honest communication is what allows AI to integrate into clinical workflows
                        sustainably. Radiologists don't expect perfection—they expect reliability and transparency. UQ
                        provides both.
                    </p>

                    <h3>What This Means for Deployment</h3>

                    <p>
                        As we discussed in our article on <a href="from-lab-to-ward.html"
                            style="color: var(--teal-primary);">AI deployment failures</a>, most medical AI never makes
                        it to the bedside. One major reason: lack of trust.
                    </p>

                    <p>
                        Models that can't communicate uncertainty fail in deployment because:
                    </p>

                    <ul>
                        <li>They generate too many false positives (eroding trust)</li>
                        <li>Radiologists can't distinguish reliable from unreliable predictions</li>
                        <li>There's no mechanism to route uncertain cases appropriately</li>
                        <li>Each confident error damages credibility</li>
                    </ul>

                    <p>
                        UQ-enabled models, by contrast, can:
                    </p>

                    <ul>
                        <li>Identify their own limitations proactively</li>
                        <li>Route cases intelligently based on confidence</li>
                        <li>Maintain trust by acknowledging uncertainty</li>
                        <li>Enable targeted review of edge cases</li>
                    </ul>

                    <hr class="article-divider-line">

                    <h2>The Bottom Line</h2>

                    <p class="article-conclusion">
                        Most FDA-cleared AI triage tools today provide binary alerts: "detected" or "not detected." No
                        confidence scores. No uncertainty measures. Often, not even a display of what was detected.
                        Every alert is treated as equally urgent, whether the model is highly confident or barely
                        crossed the detection threshold.
                    </p>

                    <p class="article-conclusion">
                        This binary approach is fundamentally flawed. When you can't distinguish confident predictions
                        from uncertain guesses, alert fatigue is inevitable. Radiologists learn to ignore flags or treat
                        them all with equal skepticism, undermining the tool's value.
                    </p>

                    <p class="article-conclusion">
                        Uncertainty quantification is the solution. It's technically feasible—the methods exist and have
                        been validated in research. What's needed is adoption: demanding that vendors add confidence
                        levels to triage alerts, and refusing to tolerate systems that treat every flag as equally
                        reliable.
                    </p>

                    <p class="article-conclusion">
                        A triage tool that says "ICH detected—highly confident" versus "ICH detected—low confidence"
                        enables smart workflow routing. High-confidence alerts get immediate attention. Low-confidence
                        alerts are reviewed but don't trigger urgent interruptions. Trust is preserved because the AI
                        acknowledges its limitations.
                    </p>

                    <p class="article-conclusion">
                        The most dangerous AI isn't the one that's sometimes wrong—it's the one that's wrong with the
                        same conviction it uses when it's right. The best triage tools are the ones that know when to
                        say "I'm not sure about this one."
                    </p>

                    <hr class="article-divider-line">

                    <!-- Key Takeaways -->
                    <div class="article-callout">
                        <h4>Key Takeaways</h4>
                        <ul style="margin: 0; padding-left: 1.5rem;">
                            <li><strong>Most Triage Tools Are Binary:</strong> FDA-cleared AI devices typically provide
                                only "detected" or "not detected" flags—no probabilities, no confidence scores, often no
                                image display.</li>
                            <li><strong>Binary Alerts Treat All Cases Equally:</strong> A barely-above-threshold
                                detection (internal score: 51%) gets the same urgent flag as an obvious case (internal
                                score: 99%). You can't tell which to trust.</li>
                            <li><strong>Alert Fatigue Is Risk:</strong> When 85% of flags are false positives and
                                all look equally urgent, radiologists learn to ignore or deprioritize alerts,
                                undermining the tool's value.</li>
                            <li><strong>UQ Would Enable Smart Triage:</strong> Confidence levels (high/medium/low) would
                                let you prioritize truly urgent alerts while treating uncertain flags appropriately.
                            </li>
                            <li><strong>Two Types of Uncertainty:</strong> Aleatoric (data ambiguity, irreducible) and
                                epistemic (knowledge gaps like out-of-distribution cases, reducible with more diverse
                                training data).</li>
                            <li><strong>Trust Requires Humility:</strong> A triage tool that can flag "high confidence"
                                vs. "low confidence" is more useful than one that treats every alert identically. The
                                best AI knows when it's uncertain.</li>
                            <li><strong>Demand Relevant Performance Data from Vendors:</strong> Ask: Does it show confidence? Can I
                                see what was detected? What's the PPV stratified by confidence level? Can I configure
                                alert behavior?</li>
                        </ul>
                    </div>

                    <!-- References -->
                    <h2>References & Further Reading</h2>

                    <ol class="article-references"
                        style="font-size: 0.95rem; line-height: 1.7; color: var(--gray-dark);">
                        <li id="ref1">
                            Faghani S, Moassefi M, Rouzrokh P, Khosravi B, Baffour FI, Ringler MD, Erickson BJ.
                            <strong>Quantifying Uncertainty in Deep Learning of Radiologic Images.</strong>
                            <em>Radiology.</em> 2023;308(2):e222217.
                            <a href="https://doi.org/10.1148/radiol.222217" target="_blank"
                                style="color: var(--teal-primary);">doi:10.1148/radiol.222217</a>
                        </li>
                        <li id="ref2">
                            Dohopolski M, Chen L, Sher D, Wang J. <strong>Predicting Lymph Node Metastasis in Patients
                                with Oropharyngeal Cancer by Using a Convolutional Neural Network with Associated
                                Epistemic and Aleatoric Uncertainty.</strong> <em>Phys Med Biol.</em>
                            2020;65(22):225002.
                            <a href="https://doi.org/10.1088/1361-6560/abc2d0" target="_blank"
                                style="color: var(--teal-primary);">doi:10.1088/1361-6560/abc2d0</a>
                        </li>
                        <li id="ref3">
                            Lee J, Shin D, Oh SH, Kim H. <strong>Method to Minimize the Errors of AI: Quantifying and
                                Exploiting Uncertainty of Deep Learning in Brain Tumor Segmentation.</strong>
                            <em>Sensors (Basel).</em> 2022;22(6):2406.
                            <a href="https://doi.org/10.3390/s22062406" target="_blank"
                                style="color: var(--teal-primary);">doi:10.3390/s22062406</a>
                        </li>
                        <li id="ref4">
                            Guo C, Pleiss G, Sun Y, Weinberger KQ. <strong>On Calibration of Modern Neural
                                Networks.</strong> <em>Proceedings of the 34th International Conference on Machine
                                Learning.</em> 2017;1321-1330.
                        </li>
                        <li id="ref5">
                            Abdar M, Pourpanah F, Hussain S, et al. <strong>A Review of Uncertainty Quantification in
                                Deep Learning: Techniques, Applications and Challenges.</strong> <em>Inf Fusion.</em>
                            2021;76:243-297.
                            <a href="https://doi.org/10.1016/j.inffus.2021.05.008" target="_blank"
                                style="color: var(--teal-primary);">doi:10.1016/j.inffus.2021.05.008</a>
                        </li>
                        <li id="ref6">
                            Ozdemir O, Russell RL, Berlin AA. <strong>A 3D Probabilistic Deep Learning System for
                                Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans.</strong> <em>IEEE Trans
                                Med Imaging.</em> 2020;39(5):1419-1429.
                            <a href="https://doi.org/10.1109/TMI.2019.2947595" target="_blank"
                                style="color: var(--teal-primary);">doi:10.1109/TMI.2019.2947595</a>
                        </li>
                        <li id="ref7">
                            Rajaraman S, Ganesan P, Antani S. <strong>Deep Learning Model Calibration for Improving
                                Performance in Class-Imbalanced Medical Image Classification Tasks.</strong> <em>PLoS
                                One.</em> 2022;17(1):e0262838.
                            <a href="https://doi.org/10.1371/journal.pone.0262838" target="_blank"
                                style="color: var(--teal-primary);">doi:10.1371/journal.pone.0262838</a>
                        </li>
                    </ol>

                </div>

                <!-- Article Footer -->
                <footer class="article-footer">
                    <div class="article-tags">
                        <span class="tag">Artificial Intelligence</span>
                        <span class="tag">Uncertainty Quantification</span>
                        <span class="tag">Clinical Decision Making</span>
                        <span class="tag">Model Trustworthiness</span>
                        <span class="tag">Radiology AI</span>
                    </div>

                    <div class="article-share">
                        <p>Share this article:</p>
                        <div class="share-buttons">
                            <a href="#" class="share-btn" aria-label="Share on Twitter">Twitter</a>
                            <a href="#" class="share-btn" aria-label="Share on LinkedIn">LinkedIn</a>
                            <a href="#" class="share-btn" aria-label="Copy link">Copy Link</a>
                        </div>
                    </div>

                    <div class="article-cta">
                        <h3>Want more insights like this?</h3>
                        <p>Deep thinking on medicine, AI, and the future clinician—delivered to your inbox.</p>
                        <a href="index.html#connect" class="btn btn-primary">Stay Connected</a>
                    </div>
                </footer>
            </div>
        </div>
    </article>

    <!-- Related Articles -->
    <section class="related-articles">
        <div class="container">
            <h2 class="section-title">Continue Reading</h2>
            <div class="articles-grid">
                <article class="article-card">
                    <div class="article-tag tag-ai">AI</div>
                    <h3 class="article-title">From Lab to Ward: Why Most Medical AI Fails at Deployment</h3>
                    <p class="article-excerpt">The validation study was flawless. The deployment was a disaster. Here's
                        why impressive AI tools never make it to the bedside—and what successful implementation actually
                        looks like.</p>
                    <div class="article-meta">
                        <span class="read-time">8 min read</span>
                        <a href="from-lab-to-ward.html" class="article-link">Read more →</a>
                    </div>
                </article>

                <article class="article-card">
                    <div class="article-tag tag-ethics">Ethics</div>
                    <h3 class="article-title">When Algorithms Inherit Bias: Fairness Challenges in Medical AI</h3>
                    <p class="article-excerpt">The algorithm is 92% accurate for white patients and 78% accurate for
                        Black patients. Do you use it? Understanding how bias propagates through AI systems.</p>
                    <div class="article-meta">
                        <span class="read-time">9 min read</span>
                        <a href="algorithm-bias.html" class="article-link">Read more →</a>
                    </div>
                </article>

                <article class="article-card">
                    <div class="article-tag tag-ai">AI</div>
                    <h3 class="article-title">Model Drift in Medicine: When AI Quietly Gets Worse</h3>
                    <p class="article-excerpt">In 2019, your sepsis model had 89% sensitivity. Today it's 71%. Nobody
                        told you. Nobody checked. The silent patient safety crisis you've never heard of.</p>
                    <div class="article-meta">
                        <span class="read-time">7 min read</span>
                        <a href="model-drift.html" class="article-link">Read more →</a>
                    </div>
                </article>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-bottom">
                    <div class="footer-links">
                        <a href="index.html">Home</a>
                        <a href="about.html">About</a>
                        <a href="index.html#articles">Thoughts on Medical AI</a>
                        <a href="papers-of-note.html">Papers of Note</a>
                        <a href="index.html#mission">Mission</a>
                    </div>
                </div>
                <div class="footer-copyright">
                    <p>&copy; 2025 NeoSynapse.md | Published at <a href="https://mdsynapse.org"
                            style="color: var(--teal-primary); text-decoration: none;">mdsynapse.org</a></p>
                </div>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>